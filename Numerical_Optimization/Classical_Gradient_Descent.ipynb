{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cdc9d98176f1dfcd1c9d8a82c18cdfee7a0231055ffa6e73384e960da528a576"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Classical Gradient Descent Method  \n",
    "\n",
    "This method is used to minimize a differentiable convex function $f$\n",
    "\n",
    "\n",
    "$$\\min _{x \\in \\mathbb{R}^{n}} f(x)$$\n",
    "\n",
    "## Algorithm: \n",
    "\n",
    "Choose a $x^0$ and repeat... \n",
    "\n",
    "$$\n",
    "x^{k-1} =x^{k}-t_{k} \\nabla f\\left(x^{k}\\right), \\mbox{ } k=0,1,2, \\ldots\n",
    "$$\n",
    "\n",
    "\n",
    "Note that $p_{k}:=-\\nabla f\\left(x^{k}\\right)$ is a descent direction. (we call $p_{k}$ a descent direction if $\\left.p_{k}^{\\top} \\nabla f_{k}<0\\right)$\n",
    "\n",
    "The question remaining is how to select an appropriate step size $t_k$?\n",
    "\n",
    "The options that we have available are... \n",
    "\n",
    "\n",
    "* Exact Line Search \n",
    "* Constant Step Size $t_k$\n",
    "* Backtracking Line Search\n",
    "\n",
    "### Exactly Line Search \n",
    "\n",
    "$$\n",
    "t_{k}=\\operatorname{argmin}_{t} f\\left(x^{k}-t \\nabla f\\left(x^{k}\\right)\\right)\n",
    "$$\n",
    "\n",
    "The \"Exact Line Search\" will in general evaluate all possible time steps $t_k$ until it find the time step which minimizes the value of the next interation $x^{k+1}$. However, this is not practical since it requires solving another optimization problem in order to solve the stepsize parameter so that we can solve the initial optimization problem.  \n",
    "\n",
    "### Fixed Time Step $t_k$\n",
    "\n",
    "Using a fixed timestep $t_k$ is acceptable however, it does not guaruntee and optimal solution, since while the constant timestep might initially lower the objective function value the actual solution to the objective function may never be reached as the constant step size might be too large or small and oscillate about the optima or get stuck in a local due to the assigned size. \n",
    "\n",
    "### Backtracking Line Search\n",
    "\n",
    "This method is used to avoid the pitfalls of a constant timestep, while not requiring the computational expense demanded by the \"exact line search\" method. In this case, initialize $t_{k}$ at some $\\hat{t}>0$ (for example, $\\hat{t}=1$ ), repeat $t_{k}:=\\beta t_{k}$ until\n",
    "$$\n",
    "f\\left(x-t_{k} \\nabla f(x)\\right)<f(x)-\\alpha t_{k}\\|\\nabla f(x)\\|_{2}^{2}\n",
    "$$\n",
    "\n",
    "This is called **Armijo's Condition** and contains two parameters $ 0 < \\beta \\mbox{ }\\& \\mbox{ } 0 < \\alpha \\leq 0.5  $\n",
    "\n",
    "This is a compromise as it performing a line search to seek a step size; however, the condition on an acceptable stepsize is eased so that the function $f$ need only be reduced by $\\alpha t_{k}\\|\\nabla f(x)\\|_{2}^{2}$ and not be the argmin stepsize which minimizes the function to the lowest value, during that timestep. \n",
    "\n",
    "Easing this condition on the criteria for an acceptable stepsize allows us to use non-constant steps while not requiring the computational expense which the \"exact\" search would incur. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-bd6b3ab6dc81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mwhile\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0merror_tolerance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;31m# counter += 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def convex_func(x):\n",
    "\n",
    "    return x**2\n",
    "\n",
    "def grad_func(x):\n",
    "    return 2*x\n",
    "\n",
    "\n",
    "# Initial Parameters\n",
    "t = 1\n",
    "x_0 = 10\n",
    "\n",
    "# Backtracking Line Search Parameters\n",
    "Beta = .8\n",
    "alpha = 0.5\n",
    "\n",
    "# Initialize Stopping Condition\n",
    "error = np.inf\n",
    "error_tolerance = .1\n",
    "counter = 0 \n",
    "time_out = 1*(10**6)\n",
    "\n",
    "# Trajectory List \n",
    "\n",
    "x_list = [x_0] \n",
    "\n",
    "\n",
    "while error >= error_tolerance:\n",
    "    # counter += 1\n",
    "\n",
    "    # if counter >= time_out:\n",
    "    #     break\n",
    "\n",
    "    # while convex_func(x_0-t*grad_func(x_0)) >= (convex_func(x_0) - alpha*t*(grad_func(x_0))**2):\n",
    "\n",
    "    #     t = Beta*t\n",
    "\n",
    "\n",
    "    x_new = x_0 - t*grad_func(x_0)\n",
    "\n",
    "    x_list.append(x_new)\n",
    "    \n",
    "    x_0 = x_new\n",
    "\n",
    "plt.plot(x_list)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "### General Line Search\n",
    "\n",
    "At $x^k$ we compute a descent direction $p_k$, and we desire to know what step size to take.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[ ] Compute Optimization Routine which optimizes cross sectional histogram, and outputs angle $\\theta$ with respect to a known reference or keyed position. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}